{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 5 Convolutional Neural Networks and Competition!\n",
    "===\n",
    "\n",
    "Inspired by:http://www.robots.ox.ac.uk/~vgg/practicals/cnn/ and \n",
    "https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html\n",
    "\n",
    "[_Convolutional neural networks_](http://www.deeplearningbook.org/contents/convnets.html) (CNNs) are specialised network type for processing data that has some continuity between adjacent components. Examples include time-series data or and image data.\n",
    "\n",
    "### CNN Building Blocks:\n",
    "A typical layer of a convolutional neural network consists of two stages. In the first stage the layer performs several convolutions (a tensor product) in parallel with non-linear activation. In the second stage we use a pooling function to modify the output to the next layer. \n",
    "\n",
    "### Convolution\n",
    "\n",
    "### Definition\n",
    "Given a two-dimensional image, **$I$**, and a small matrix **$K$** of size $h \\times w$, (known as _convolution kernel_), which encodes a way of extracting an intersting image feature, we compute the convolved image $I*K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of the elementwise products between the image and the kernel:\n",
    "\\begin{equation}\n",
    "(I*K)_{xy} = \\sum_{i=1}^h \\sum_{j=1}^w K_{ij} \\cdot I_{x+i-1,y+j-1}\n",
    " \\end{equation}\n",
    " \n",
    "![](https://i.imgur.com/zixMZWO.gif)\n",
    "\n",
    "Convolution takes advantage of the structure of the information encoded in the image - for example, pixels that are spatially closer will 'cooperate' in forming a feature more than pixels that are on opposite corners. In addition, if there is an important feature for defining the image's label, it will be equally important if this feature is found in the image, regardless of the location.\n",
    "\n",
    "\n",
    "### Pooling\n",
    "A pooling function replaces the output of the network at a certain location with a summary statistic of the nearby outputs. For example, the **max pooling** operation reports the maximum output within a rectangular neighbourhood as shown below. Other pooling functions include the average of a rectangular neighbourhood and a weighted average based on the distance from the central pixel.\n",
    "![](https://i.imgur.com/eg8q5Mr.png)\n",
    "Pooling helps to make the representation approximately invariant to small translations of the input. Invariance to translation means that if we shift the image by a few pixels, the values of the most of the pooled outputs do not change. For example, when determining whether an image contains a face, we do not need to know the precise location of the eyes, we just need to know that there is an eye on the left side of the face and another eye on the right side.\n",
    "\n",
    "### Back-propagation\n",
    "Like standard neural networks, CNN are trained by optimising their parameters such as to minimise a cost function. This process is usually down using a gradient based optimisation algorithm such as Stochastic Gradient Descent. Like in standard neural networks, back-probagation is used to find the gradient of the cost function of the CNN in terms of its parameters. Since, calculating the gradients using back-propagation can be a very involved process, especially in deep architecture, modern neural network software packages such as Tensorflow and Theano represent neural networks as computational graphs thus enabling back-propagation to be performed automatically and efficiently.\n",
    "\n",
    "### Putting It All Together: A Common CNN\n",
    "A typical CNN architecture consists of multiple alternating convolutional and pooling layers followed by a set of dense (fully-connected layers). The image below shows an example of a standard CNN architecutre.\n",
    "\n",
    "![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-07-at-7.26.20-AM.png)\n",
    "\n",
    "The intuition behind CNN architectures such as the one above is that the first few layers learn low-level features such as edges and texture, while the deeper layers learn more high-level features that are dataset specific such as typical object shape or facial features. This idea is illustrated in the figure below.\n",
    "![](http://vision03.csail.mit.edu/cnn_art/data/single_layer.png)\n",
    "\n",
    "### CNN  Example - CIFAR-10:\n",
    "We are going to demonstrate CNNs on the CIFAR-10 classification task. CIFAR-10 is a dataset consisting of 60000 32x32 RGB images each belonging to one of 10 classes. Here is a sample of the images.\n",
    "\n",
    "![](http://karpathy.github.io/assets/cifar_preview.png)\n",
    "\n",
    "We are going to train a CNN to classify images from this dataset into their correct classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exlporing the dataset:\n",
    "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to automatically download and load the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (50000, 32, 32, 3)\n",
      "Number of training examples: 50000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "((train_X, train_Y), (test_X, test_Y)) = cifar10.load_data()\n",
    "print('train_X shape:', train_X.shape)\n",
    "print('Number of training examples:', train_X.shape[0])\n",
    "print('Number of test examples:', test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 classes:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(train_Y)\n",
    "num_classes = classes.shape[0]\n",
    "print('There are', num_classes, 'classes:')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_dictionary = {\n",
    "    0:'airplane',\n",
    "    1:'automobile',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view random samples from the dataset using the code below. Feel free to change the value of `index` to view different samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is class: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1264f4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 123\n",
    "print('This is class:', class_dictionary[train_Y[index][0]])\n",
    "plt.imshow(train_X[index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our dataset to be compatible with the Keras API, we nee our labels to be represented using [one-hot encoding](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science). We also want to convert the image representation from 8-bit (integer) rgb pixels to 32-bit floating points between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class label to one-hot encoded vector\n",
    "train_Y = keras.utils.to_categorical(train_Y, num_classes)\n",
    "test_Y = keras.utils.to_categorical(test_Y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the images to 32-bit float \n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "# Normalise the images from 0 to 1\n",
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_XV=train_X.reshape(50000,3072)\n",
    "test_XV=test_X.reshape(10000,3072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example of a standard network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 197,322\n",
      "Trainable params: 197,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Dense(64,input_dim=3072,activation='relu'))\n",
    "model1.add(Dense(10,activation='softmax'))\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 3s - loss: 2.1028 - acc: 0.2509     \n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 3s - loss: 1.9013 - acc: 0.3227     \n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s - loss: 1.8179 - acc: 0.3528     \n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 3s - loss: 1.7684 - acc: 0.3721     \n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s - loss: 1.7367 - acc: 0.3831     \n",
      " 9568/10000 [===========================>..] - ETA: 0s\n",
      "\n",
      "Number correctly estimated on test set 37.9 %\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_XV,train_Y,epochs=5,batch_size=128);\n",
    "score=model1.evaluate(test_XV,test_Y,batch_size=32);\n",
    "print(\"\\n\\nNumber correctly estimated on test set\",100*score[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network that improves accuracy\n",
    "Modification of Ayman's and Iliana's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same',input_shape=train_X.shape[1:],activation='relu'))\n",
    "model.add(Conv2D(16, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                230464    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 233,882\n",
      "Trainable params: 233,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise SGD optimiser\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 69s - loss: 1.6416 - acc: 0.4127    \n",
      "Epoch 2/5\n",
      " 1408/50000 [..............................] - ETA: 71s - loss: 1.3811 - acc: 0.5135"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f899fc30c81c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nNumber correctly estimated \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/games-and-theory/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y,batch_size=128,epochs=2,shuffle=True)\n",
    "score=model.evaluate(test_X,test_Y,batch_size=32);\n",
    "print(\"\\n\\nNumber correctly estimated \",100*score[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
