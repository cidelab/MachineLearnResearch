{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 5 Convolutional Neural Networks and Competition!\n",
    "===\n",
    "\n",
    "Inspired by:http://www.robots.ox.ac.uk/~vgg/practicals/cnn/ and \n",
    "https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html\n",
    "\n",
    "[_Convolutional neural networks_](http://www.deeplearningbook.org/contents/convnets.html) (CNNs) are specialised network type for processing data that has some continuity between adjacent components. Examples include time-series data or and image data.\n",
    "\n",
    "### CNN Building Blocks:\n",
    "A typical layer of a convolutional neural network consists of two stages. In the first stage the layer performs several convolutions (a tensor product) in parallel with non-linear activation. In the second stage we use a pooling function to modify the output to the next layer. \n",
    "\n",
    "### Convolution\n",
    "\n",
    "### Definition\n",
    "Given a two-dimensional image, **$I$**, and a small matrix **$K$** of size $h \\times w$, (known as _convolution kernel_), which encodes a way of extracting an intersting image feature, we compute the convolved image $I*K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of the elementwise products between the image and the kernel:\n",
    "\\begin{equation}\n",
    "(I*K)_{xy} = \\sum_{i=1}^h \\sum_{j=1}^w K_{ij} \\cdot I_{x+i-1,y+j-1}\n",
    " \\end{equation}\n",
    " \n",
    "![](https://i.imgur.com/zixMZWO.gif)\n",
    "\n",
    "Convolution takes advantage of the structure of the information encoded in the image - for example, pixels that are spatially closer will 'cooperate' in forming a feature more than pixels that are on opposite corners. In addition, if there is an important feature for defining the image's label, it will be equally important if this feature is found in the image, regardless of the location.\n",
    "\n",
    "\n",
    "### Pooling\n",
    "A pooling function replaces the output of the network at a certain location with a summary statistic of the nearby outputs. For example, the **max pooling** operation reports the maximum output within a rectangular neighbourhood as shown below. Other pooling functions include the average of a rectangular neighbourhood and a weighted average based on the distance from the central pixel.\n",
    "![](https://i.imgur.com/eg8q5Mr.png)\n",
    "Pooling helps to make the representation approximately invariant to small translations of the input. Invariance to translation means that if we shift the image by a few pixels, the values of the most of the pooled outputs do not change. For example, when determining whether an image contains a face, we do not need to know the precise location of the eyes, we just need to know that there is an eye on the left side of the face and another eye on the right side.\n",
    "\n",
    "### Back-propagation\n",
    "Like standard neural networks, CNN are trained by optimising their parameters such as to minimise a cost function. This process is usually down using a gradient based optimisation algorithm such as Stochastic Gradient Descent. Like in standard neural networks, back-probagation is used to find the gradient of the cost function of the CNN in terms of its parameters. Since, calculating the gradients using back-propagation can be a very involved process, especially in deep architecture, modern neural network software packages such as Tensorflow and Theano represent neural networks as computational graphs thus enabling back-propagation to be performed automatically and efficiently.\n",
    "\n",
    "### Putting It All Together: A Common CNN\n",
    "A typical CNN architecture consists of multiple alternating convolutional and pooling layers followed by a set of dense (fully-connected layers). The image below shows an example of a standard CNN architecutre.\n",
    "\n",
    "![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-07-at-7.26.20-AM.png)\n",
    "\n",
    "The intuition behind CNN architectures such as the one above is that the first few layers learn low-level features such as edges and texture, while the deeper layers learn more high-level features that are dataset specific such as typical object shape or facial features. This idea is illustrated in the figure below.\n",
    "![](http://vision03.csail.mit.edu/cnn_art/data/single_layer.png)\n",
    "\n",
    "### CNN  Example - CIFAR-10:\n",
    "We are going to demonstrate CNNs on the CIFAR-10 classification task. CIFAR-10 is a dataset consisting of 60000 32x32 RGB images each belonging to one of 10 classes. Here is a sample of the images.\n",
    "\n",
    "![](http://karpathy.github.io/assets/cifar_preview.png)\n",
    "\n",
    "We are going to train a CNN to classify images from this dataset into their correct classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exlporing the dataset:\n",
    "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to automatically download and load the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (50000, 32, 32, 3)\n",
      "Number of training examples: 50000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "((train_X, train_Y), (test_X, test_Y)) = cifar10.load_data()\n",
    "print('train_X shape:', train_X.shape)\n",
    "print('Number of training examples:', train_X.shape[0])\n",
    "print('Number of test examples:', test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 classes:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(train_Y)\n",
    "num_classes = classes.shape[0]\n",
    "print('There are', num_classes, 'classes:')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_dictionary = {\n",
    "    0:'airplane',\n",
    "    1:'automobile',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view random samples from the dataset using the code below. Feel free to change the value of `index` to view different samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is class: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x237e87752e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 140\n",
    "print('This is class:', class_dictionary[train_Y[index][0]])\n",
    "plt.imshow(train_X[index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our dataset to be compatible with the Keras API, we nee our labels to be represented using [one-hot encoding](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science). We also want to convert the image representation from 8-bit (integer) rgb pixels to 32-bit floating points between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class label to one-hot encoded vector\n",
    "train_Y = keras.utils.to_categorical(train_Y, num_classes)\n",
    "test_Y = keras.utils.to_categorical(test_Y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the images to 32-bit float \n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "# Normalise the images from 0 to 1\n",
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_XV=train_X.reshape(50000,3072)\n",
    "test_XV=test_X.reshape(10000,3072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example of a standard network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 197,322\n",
      "Trainable params: 197,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Dense(64,input_dim=3072,activation='relu'))\n",
    "model1.add(Dense(10,activation='softmax'))\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 2s - loss: 2.1245 - acc: 0.2334     \n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1s - loss: 1.8824 - acc: 0.3249     \n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 2s - loss: 1.8041 - acc: 0.3559     \n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 2s - loss: 1.7589 - acc: 0.3721     \n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 2s - loss: 1.7245 - acc: 0.3855     \n",
      " 8928/10000 [=========================>....] - ETA: 0s\n",
      "\n",
      "Number correctly estimated on test set 37.86 %\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_XV,train_Y,epochs=5,batch_size=128);\n",
    "score=model1.evaluate(test_XV,test_Y,batch_size=32);\n",
    "print(\"\\n\\nNumber correctly estimated on test set\",100*score[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network that improves accuracy\n",
    "Modification of Ayman's and Iliana's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same',input_shape=train_X.shape[1:],activation='relu'))\n",
    "model.add(Conv2D(16, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                230464    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 233,882\n",
      "Trainable params: 233,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise SGD optimiser\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 43s - loss: 1.6905 - acc: 0.3965    \n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 44s - loss: 1.3359 - acc: 0.5293    \n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 43s - loss: 1.1787 - acc: 0.5872    \n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 42s - loss: 1.0713 - acc: 0.6279    \n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 42s - loss: 0.9919 - acc: 0.6543    \n",
      " 9952/10000 [============================>.] - ETA: 0s\n",
      "\n",
      "Number correctly estimated  60.01 %\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y,batch_size=128,epochs=5,shuffle=True)\n",
    "score=model.evaluate(test_X,test_Y,batch_size=32);\n",
    "print(\"\\n\\nNumber correctly estimated \",100*score[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload the data again to strip down to 4 classes\n",
    "((train_X, train_Y), (test_X, test_Y)) = cifar10.load_data()\n",
    "\n",
    "# Convert the images to 32-bit float \n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "# Normalise the images from 0 to 1\n",
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "\n",
    "\n",
    "# fix the training set\n",
    "s0=np.where(train_Y==0)[0] # 0 airplanes\n",
    "s1=np.where(train_Y==1)[0] # 1 automobiles\n",
    "s2=np.where(train_Y==3)[0] # 2 cats\n",
    "s3=np.where(train_Y==7)[0] # 3 horses\n",
    "\n",
    "train_Y[s2]=2\n",
    "train_Y[s3]=3\n",
    "s=np.hstack([s0,s1,s2,s3])\n",
    "mix=np.random.permutation(s.shape[0])\n",
    "smix=s[mix]\n",
    "\n",
    "train_X4=train_X[smix,:,:,:]\n",
    "train_Y4=train_Y[smix]\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "for k in range(6):\n",
    "    plt.subplot(1,6,k+1)\n",
    "    plt.imshow(train_X4[k,:,:,:])\n",
    "    plt.title(train_Y4[k])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# fix the test set \n",
    "s0=np.where(test_Y==0)[0] # 0 airplanes\n",
    "s1=np.where(test_Y==1)[0] # 1 automobiles\n",
    "s2=np.where(test_Y==3)[0] # 2 cats\n",
    "s3=np.where(test_Y==7)[0] # 3 horses\n",
    "\n",
    "test_Y[s2]=2\n",
    "test_Y[s3]=3\n",
    "s=np.hstack([s0,s1,s2,s3])\n",
    "mix=np.random.permutation(s.shape[0])\n",
    "smix=s[mix]\n",
    "\n",
    "test_X4=test_X[smix,:,:,:]\n",
    "test_Y4=test_Y[smix]\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "for k in range(6):\n",
    "    plt.subplot(1,6,k+1)\n",
    "    plt.imshow(test_X4[k,:,:,:])\n",
    "    plt.title(test_Y4[k])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "# change to one-hot encoding for the target    \n",
    "train_Y4 = keras.utils.to_categorical(train_Y4, 4)\n",
    "test_Y4 = keras.utils.to_categorical(test_Y4, 4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training data is train_X4, train_Y3\n",
    "* Test data is test_X4, test_Y4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q1. The competition!\n",
    "\n",
    "We will use the stripped down version of cifar10 that contains only 4 classes.\n",
    "\n",
    "* Construct a neural network that gets the highest classification on the test set.\n",
    "* The group with the highest percentage wins!\n",
    "* ***Do not use the test set for training*** - we will check the winning submission.\n",
    "* Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3),padding='same',input_shape=train_X4.shape[1:],activation='relu'))\n",
    "model.add(Conv2D(16, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise SGD optimiser\n",
    "from keras.layers import Input\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 20 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons\n",
    "\n",
    "num_train, height, width, depth = train_X4.shape\n",
    "\n",
    "inp = Input(shape=(height, width, depth))\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Conv2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(inp)\n",
    "conv_2 = Conv2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Conv2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Conv2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "\n",
    "model = model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=0.1)\n",
    "opt1 = keras.optimizers.Adam(lr=0.001, beta_1=0.93, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_X4, train_Y4,batch_size=256,epochs=5,shuffle=True)\n",
    "\n",
    "score=model.evaluate(test_X4,test_Y4,batch_size=32);\n",
    "\n",
    "print(\"\\n\\nNumber correctly estimated \",100*score[1],\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
