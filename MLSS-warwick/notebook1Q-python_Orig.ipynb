{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part I \n",
    "\n",
    "We will write code that performs logistic regression on linearly separable data, by learning weights through gradient descent of a cost function. This is a simple problem but it will allow us to understand better how a network with one hidden layer works in Part II. \n",
    "\n",
    "* The network required has an input layer of two neurons plus a bias and an output layer of one neuron. So the data matrix $\\mathbf{X}$ has dimensions $(n_s,2)$, where $n_s$ is the number of samples. The target matrix $\\mathbf{T}$, containing ones or zeros depending on the classification of the data, has dimensions $(n_s,1)$.\n",
    "* On including the bias node (set to unity) we increase the column size of the data matrix by one and call this matrix $\\mathbf{\\tilde{X}}$.\n",
    "* There is a weight matrix (just a vector in this case) $\\mathbf{w}$ that has dimensions $(3,1)$.\n",
    "* The activation function of the output neuron is a sigmoid function of its input $z$ such that $f(z)=1/(1+\\exp(-z))$ where for one data point $z=\\sum_i\\tilde{x}_iw_i$ is a linear sum of the weighted inputs.\n",
    "* The matrix of predictions of the network is therefore \n",
    "$$\\mathbf{P}=f(\\mathbf{\\tilde{X}\\mathbf{w}})$$\n",
    "* The average cost function across the data is $$C=-\\frac{1}{n_s}\\sum_s\\left[T_s\\log(P_s)+(1-T_s)\\log(1-P_s)\\right]. $$\n",
    "\n",
    "* As a general convention we will use upper-case letters when considering matricies of all data and lower-case letters for aone single data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries required for plotting and maths\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Generate the data and the target\n",
    "\n",
    "* Each data point comprises two numbers, both randomly distributed between $-1$ and $1$, which can be intepreted as an $(x, y)$ coordinate on a plane.\n",
    "* If the point obeys $y>mx+\\kappa$ we assign it a target value $t=1$, if not it has $t=0$.\n",
    "* For $n_s$ samples the data can be summarised by two matricies. The first $\\mathbf{X}$ has the coordinates and dimensions $(n_s,2)$ and the second $\\mathbf{T}$ has the targets and has dimensions $(n_s,1)$.\n",
    "* Generate these matricies and plot the points in green that have $t=1$ and red that have $t=0$. Draw the dividing line given by $y=mx+\\kappa$.\n",
    "* Suggested parameters are $n_s=50$, $m=1$ and $\\kappa=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Write a function that calculates a prediction and cost for a given set of weights\n",
    "* We now include a bias so we define a new matrix $\\tilde{\\mathbf{X}}$ that includes a column of ones. This matrix has dimensions $(n_s,3)$.\n",
    "* The weight matrix $w$ has dimensions ($3,1)$ and stores the information learned about the data.\n",
    "* The prediction matrix $\\mathbf{P}$ has the same dimensions as $\\mathbf{T}$ and is given by $\\mathbf{P}=f(\\tilde{\\mathbf{X}}\\mathbf{w})$ where $f(z)=1/(1+e^{-z})$.\n",
    "* The cost function is $$C=-\\frac{1}{n_s}\\sum_s\\left[T_s\\log(P_s)+(1-T_s)\\log(1-P_s)\\right]$$\n",
    "* Write a function that takes $\\tilde{\\mathbf{X}}$ and a weight matrix $\\mathbf{w}$ as inputs and outputs the prediction matrix $\\mathbf{P}$ and cost $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Write a function that calculates the weight gradients\n",
    "We first do a little theory to calculate the form of the cost gradient with respect to changes in the weight. For simplicity we will consider a single data point first.\n",
    "* Let the cost of one sample be\n",
    "$$c=-[t\\log(p)+(1-t)\\log(1-p)].$$ \n",
    "Show that \n",
    "$$ \\frac{dc}{dp}=\\frac{p-t}{p(1-p)},~~~\\frac{dp}{dz}=p(1-p) ~~~\\mbox{ and } \\frac{dz}{dw_i}=\\tilde{x}_i$$\n",
    "* Hence show that $$\\frac{dc}{dw_i}=\\frac{dp}{dz}\\frac{dz}{dw_i}=\\tilde{x}_i(p-t) ~~\\mbox{ and }~~\\frac{dC}{dw_i}=\\frac{1}{n_s}\\sum_s\\tilde{X}_{si}(P_{si}-T_{si}) ~~\\mbox{for the entire data set}.$$ \n",
    "* In matrix form we therefore have $$\\frac{dC}{d\\mathbf{w}}=\\frac{1}{n_s}\\tilde{X}'\\Delta^p~~~\\mbox{ where }~~~\\Delta^p=\\mathbf{P}-\\mathbf{T} ~~~\\mbox{ or }~~~\\delta^p=p-t~~~\\mbox{for one data point}$$ \n",
    "* Write a function that takes in $\\mathbf{\\tilde{X}}$, $\\mathbf{w}$, $\\mathbf{P}$ and $\\mathbf{T}$ and outputs $dC/d\\mathbf{w}$ as a matrix  of dimension $(1,3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Gradient descent for the weights\n",
    "* Write a loop that does $n_i$ gradient-descent iterations, where for each one the weight matrix is updated using the rule $$ \\mathbf{w}_\\mathrm{new}=\\mathbf{w}_\\mathrm{old}-\\alpha\\frac{dC}{d\\mathbf{w}} $$ where $\\alpha$ is a positive learning rate that you can choose.\n",
    "* Plot a graph of the cost $C$ as a function of the iteration steps and check that it is monotonically decreasing.\n",
    "* When the input $z$ into the output node vanishes then $p=0.5$ which defines the separation line. The criterion $z=0$ corresponds to $xw_1+yw_2+w_3=0$ and defines a line separating the classification in the $x-y$ plane. Plot the line for your weights on the graph of data points and compare it to the true separting line used to generate the data labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}