{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 2. Network with a hidden layer\n",
    "\n",
    "Aim is to extend the model of notebook 1 to a neural network with one hidden layer. \n",
    "* The matricies $\\mathbf{X}$, $\\mathbf{\\tilde{X}}$, $\\mathbf{T}$ are generated as for the simple input-output network and $\\mathbf{P}$ will denote the prediction matrix.\n",
    "* The hidden layer has $n_h$ units as well as a bias. The matrix of its values for all samples is $\\mathbf{H}$ with dimension $(n_s,n_h)$. An additional matrix $\\mathbf{\\tilde{H}}$ is defined that includes the bias as a column of ones, and has dimension $(n_s,n_h+1)$. \n",
    "* The matrix of weights $\\mathbf{w}$ between layers $\\tilde{X}$ and $H$ has dimensions $(3,n_h)$\n",
    "* The matrix of weights $\\mathbf{v}$ between layers $\\tilde{H}$ and $P$ has dimensions $(n_h+1,1)$.\n",
    "* The non-linearities are again given by sigmoidal activations of the linear input $z$ so that $f(z)=1/(1+\\exp(-z))$. The prediction of the network can therefore be written in matrix form as\n",
    "$$\n",
    "f(\\tilde{\\mathbf{X}}\\mathbf{w})=\\mathbf{H}~~\\mbox{ and} ~~f(\\tilde{\\mathbf{H}}\\mathbf{v})=\\mathbf{P}.\n",
    "$$\n",
    "* The cost function is the same as the previous case\n",
    "$$C=-\\frac{1}{n_s}\\sum_s\\left[T_s\\log(P_s)+(1-T_s)\\log(1-P_s)\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries required for plotting and maths\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Generate the x,y data\n",
    "* This time the data will not be separable by a straight line. \n",
    "* Consider two circles centred at (0.3,0.3) and (-0.5,0.5) both of radius 0.3.\n",
    "* Generate $n_s=400$ points on the plane distributed as in the previous case.\n",
    "* If the points fall within either circle they are assigned $t=1$, otherwise $t=0$.\n",
    "* Generate the matricies $\\mathbf{X}$ and $\\mathbf{T}$ and plot out the scatter of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Write a function that calculates a prediction and cost for a given set of weights\n",
    "* The input-to-output transformations are $$ f(\\tilde{\\mathbf{X}}\\mathbf{w})=\\mathbf{H}~~\\mbox{ and} ~~f(\\tilde{\\mathbf{H}}\\mathbf{v})=\\mathbf{P}$$ \n",
    "where $f(z)=1/(1+e^{-z})$.\n",
    "* The cost function is the same as before \n",
    "$$C=-\\frac{1}{n_s}\\sum_s\\left[T_s\\log(P_s)+(1-T_s)\\log(1-P_s)\\right]$$\n",
    "* Creat a function that takes $\\tilde{\\mathbf{X}}$, $\\mathbf{w}$, $\\mathbf{v}$ and $\\mathbf{T}$ as inputs, and then outputs the hidden layer $\\mathbf{\\tilde{H}}$, prediction matrix $\\mathbf{P}$ and cost $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Write a function that calculates the weight gradients\n",
    "* Using similar arguments to the simple input-output network show that\n",
    "$$ \\frac{dc}{dv_j}=\\tilde{h}_j\\delta^p~~~\\mbox{ so that in matrix form}~~~ \n",
    "\\frac{dC}{d\\mathbf{v}}=\\frac{1}{n_s}\\tilde{\\mathbf{H}}'\\Delta^p\n",
    "$$\n",
    "is the gradient for the $\\mathbf{v}$ weights, where $\\delta^p=(p-t)$ and $\\Delta^p_s$ is a matrix of dimension $(n_s,1)$. \n",
    "* Show that the cost gradient for $\\mathbf{w}$ for a data point obeys\n",
    "$$\n",
    "\\frac{dc}{dw_{ij}}=\\frac{dc}{dp}\\frac{dp}{dz^p}\\frac{dz^p}{dh_j}\\frac{dh_j}{dw_{ij}}=\\tilde{x}_i\\delta^h_j ~~~\\mbox{ where }~~~\\delta_j^h=h_j(1-h_j)\\delta^pv_j~~~\\mbox{ and $j$ runs from 1 to $n_h$}\n",
    "$$\n",
    "* This gives the gradient for the $\\mathbf{w}$ weights as\n",
    "$$\n",
    "\\frac{dC}{d\\mathbf{w}}=\\frac{1}{n_s}\\tilde{\\mathbf{X}}'\\Delta^h\n",
    "$$\n",
    "where the $(n_s,n_h)$ dimensional matrix $\\Delta^h$ has entries that are\n",
    "$$\n",
    "\\Delta^h_{sj}=H_{sj}(1-H_{sj})[\\Delta_s^pv_j]\n",
    "$$\n",
    "* Write a function that takes in $\\mathbf{\\tilde{X}}$, $\\mathbf{\\tilde{H}}$, $\\mathbf{v}$, $\\mathbf{P}$ and $\\mathbf{T}$ and outputs $dC/d\\mathbf{w}$ and $dC/d\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Gradient descent for the weights\n",
    "* Write a loop that does $n$ iterations of the gradient descent, where for each one the weight matricies are updated \n",
    "$$ \\mathbf{w}_\\mathrm{new}=\\mathbf{w}_\\mathrm{old}-\\alpha\\frac{dC}{d\\mathbf{w}} \n",
    "~~~\\mbox{ and }~~~\n",
    "\\mathbf{v}_\\mathrm{new}=\\mathbf{v}_\\mathrm{old}-\\alpha\\frac{dC}{d\\mathbf{v}} \n",
    "$$ where $\\alpha$ is a positive learning rate that you can choose.\n",
    "* Plot a graph of the cost $C$ as a function of the iteration steps and check that it is monotonically decreasing.\n",
    "* How does the cost vary with different choices of $n_h$?\n",
    "* What is the percentage of correctly classified data points in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Plotting the classifier and hidden layers\n",
    "\n",
    "This data is not linearly separable; however, you can interpret the weights coming into the hidden layer as defining $n_h$ lines, like was done for the simple input-output case in notebook 1. \n",
    "* Plot the $n_h$ lines represented by the weight matrix $\\mathbf{w}$ to gain insight into the role of the hidden layer.\n",
    "* Plot a grid of points on the plane that are colour coded by the predictions. This can be achieved by using one of the previous functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Comparison with a test set of data\n",
    "An additional question if you have time.\n",
    "* Generate a test set of data and compare the classification of the already trained network with those data.\n",
    "* How do the percentages of training (first set) and test set vary as you increase the number of hidden units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
